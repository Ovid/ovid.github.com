
<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-BVDP76N9NB"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-BVDP76N9NB');
  </script>
  
  <!-- Facebook -->

  <meta property="fb:app_id" content="329861788447703">
  
  <meta property="og:image" content="https://curtispoe.org/static/images/facebook/ovid-facebook.jpg">
  <meta property="og:image:alt" content="A black and white image of the author, Curtis “Ovid” Poe.">
  
  <meta property="og:type" content="article">
  
  <meta property="og:url" content="https://curtispoe.org/articles/zen-of-test-suites.html">
  
  <meta property="og:title" content="The Zen of Test Suites">
  <meta property="og:description" content="An overview of some basic test suite tips that are nonetheless overlooked.">

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="canonical" href="https://curtispoe.org/articles/zen-of-test-suites.html">
  

  <!-- Basic Page Needs -->
  <meta charset="utf-8">
  <title>The Zen of Test Suites</title>
  <meta name="description" content="The Zen of Test Suites">
  <meta name="author" content="Curtis Poe">
  <link rel="alternate" type="application/rss+xml" title="Subscribe to my technical blog" href="https://curtispoe.org/article.rss">
  <link rel="alternate" type="application/rss+xml" title="Subscribe to my personal blog" href="https://curtispoe.org/blog.rss">

  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS -->
  <link rel="stylesheet" href="/static/css/normalize.css">
  <link rel="stylesheet" href="/static/css/skeleton.css">
  <link rel="stylesheet" href="/static/css/main.css">
  <link rel="stylesheet" href="/static/css/dialog.css">
  <link rel="stylesheet" href="/static/css/image.css">
    

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  
  <!-- Favicon -->
  <link rel="icon" type="image/png" href="images/favicon.png">

</head>
<body>

  <!-- Primary Page Layout -->
  <div class="container">
    <div class="row books">
        <div class="twelve columns header">
            <ul>
              <li><a href="https://www.amazon.com/Perl-Hacks-Programming-Debugging-Surviving/dp/0596526741/" target="_blank"><img src="/static/images/perl-hacks.jpg" alt="The cover of the 'Perl Hacks' book" class="book"></a></li>
              <li><a href="https://www.amazon.com/Beginning-Perl-Curtis-Poe/dp/1118013840/" target="_blank"><img src="/static/images/beginning-perl.jpg" alt="The cover of the 'Beginning Perl' book" class="book"></a></li>
              <li><img class="book" src="/static/images/profile.png" alt="An image of Curtis Poe, holding some electronic equipment in front of his face."></li>
            </ul>
        </div>
    </div>
    <div class="row title">
        <!-- Back to top button -->
        <span aria-hidden="true"><a href="#top" class="arrow"><button id="scrollToTopButton">↑</button></a></span>
        <h1><a name="-title-no-title-found-"></a>The Zen of Test Suites</h1>
        <time>2018-09-16</time>
        
        <p><span id="time"></span> minute read</p>
        
        <hr>
        <div class="twelve columns header">
        </div>
    </div>
    <div class="row">
      
      <div class="two columns">
          <span id="wasm_search"></span>
  <!-- Note the usage of `type=module` here as this is an ES6 module -->
  <script type="module">
    // Use ES module import syntax to import functionality from the module
    // that we have compiled.
    //
    // Note that the `default` import is an initialization function which
    // will "boot" the module and make it ready to use. Currently browsers
    // don't support natively imported WebAssembly as an ES module, but
    // eventually the manual initialization won't be required!
    // import { search, default as init } from './tinysearch_engine.js';
    import { search, default as init } from '/static/js/search/tinysearch_engine.js';
    window.search = search;

    async function run() {
      // First up we need to actually load the wasm file, so we use the
      // default export to inform it where the wasm file is located on the
      // server, and then we wait on the returned promise to wait for the
      // wasm to be loaded.
      //
      // Note that instead of a string here you can also pass in an instance
      // of `WebAssembly.Module` which allows you to compile your own module.
      // Also note that the promise, when resolved, yields the wasm module's
      // exports which is the same as importing the `*_bg` module in other
      // modes
      await init('/static/js/search/tinysearch_engine_bg.wasm');
    }

    run();
  </script>

  <script>
    // And afterwards we can use all the functionality defined in wasm.
    function doSearch() {
      let value = document.getElementById("demo").value;
      console.log(`Search query: ${value}`);

      const results = search(value, 5);

      console.log(`Results: ${results}`);

      let ul = document.getElementById("results");
      ul.innerHTML = "";

      for (i = 0; i < results.length; i++) {
        var li = document.createElement("li");

        let [title, url] = results[i];
        let elemlink = document.createElement('a');
        elemlink.innerHTML = title;
        elemlink.setAttribute('href', url);
        li.appendChild(elemlink);

        ul.appendChild(li);
      }
    }
	// https://stackoverflow.com/questions/47879864/how-can-i-check-if-a-browser-supports-webassembly#:~:text=There%20are%20a%20few%20ways,js).
	const wasm_supported = (() => {
		try {
			if (typeof WebAssembly === "object"
				&& typeof WebAssembly.instantiate === "function") {
				const module = new WebAssembly.Module(Uint8Array.of(0x0, 0x61, 0x73, 0x6d, 0x01, 0x00, 0x00, 0x00));
				if (module instanceof WebAssembly.Module)
					return new WebAssembly.Instance(module) instanceof WebAssembly.Instance;
			}
		} catch (e) {
		}
		return false;
	})();
    if (!wasm_supported) {
      // don't even show them the search box if they don't have web assembly
      // document.getElementById("wasm_search").innerHTML = "Your browser does not support WebAssembly. Please use a modern browser.";
    }
    else {
      document.getElementById("wasm_search").innerHTML = '<div id="search"><strong>Search</strong><input type="text" id="demo" onkeyup="doSearch()"><ul id="results"></ul><div><hr>';
    }
  </script>

        <ul>
          <li><a href="/index.html">Home</a></li>
          <li><a href="/articles.html">Articles</a> <a href="/article.rss"><img border="0" alt="Subscribe to Articles by Ovid" src="/static/images/rss.png" width="12" height="12"/></a></li>
          <li><a href="/blog.html">Blog</a> <a href="/blog.rss"><img border="0" alt="Subscribe to Blogs by Ovid" src="/static/images/rss.png" width="12" height="12"/></a></li>
          <li><a href="/videos.html">Talks</a></li>
          <li><a href="/hireme.html">Hire Me</a></li>
          <li><a href="/wildagile.html">WildAgile</a></li>
          <!-- <li><a href="/tau-station.html">Tau Station</a></li> -->
          <li><a href="/starmap.html">Starmap</a></li>
          <li><a href="/escape.html"><strong>Escape!</strong></a></li>
        </ul>
        <hr>
        <strong>Find me on ...</strong>
        <ul>
          <li><a href="https://www.linkedin.com/in/curtispoe/">LinkedIn</a></li>
          <li><a href="https://github.com/Ovid/">GitHub</a></li>
          <li><a href="https://fosstodon.org/@ovid" rel="me">Mastodon</a></li>
          <li><a href="https://bsky.app/profile/ovid.bsky.social">Bluesky</a></li>
        </ul>
        <strong>Tags</strong>
        <ul class="cloud" role="navigation" aria-label="Tag cloud for Ovid's site">
        
            <li><a href="/tags/programming.html" data-weight="9">Software</a></li>
        
            <li><a href="/tags/business.html" data-weight="7">Business</a></li>
        
            <li><a href="/tags/ai.html" data-weight="5">AI</a></li>
        
            <li><a href="/tags/oop.html" data-weight="5">OOP</a></li>
        
            <li><a href="/tags/perl.html" data-weight="5">Perl</a></li>
        
            <li><a href="/tags/corinna.html" data-weight="4">Corinna</a></li>
        
            <li><a href="/tags/writing.html" data-weight="4">Writing</a></li>
        
            <li><a href="/tags/personal.html" data-weight="3">Personal</a></li>
        
            <li><a href="/tags/politics.html" data-weight="3">Politics</a></li>
        
            <li><a href="/tags/space.html" data-weight="3">Space</a></li>
        
            <li><a href="/tags/databases.html" data-weight="2">Databases</a></li>
        
            <li><a href="/tags/family.html" data-weight="2">Family</a></li>
        
            <li><a href="/tags/math.html" data-weight="2">Math</a></li>
        
            <li><a href="/tags/science.html" data-weight="2">Science</a></li>
        
            <li><a href="/tags/expat.html" data-weight="1">Moving Abroad</a></li>
        
        </ul>
      </div>

        <div class="ten columns verticalLine article">

        <div class="prevNext">
        
        <a href="/articles/how-databases-can-hurt-your-startup.html" class="prevPost">&laquo; How the database can hurt your startup</a>
        
        <a href="/articles/death-by-database.html" class="nextPost">Death by Database &raquo;</a>
    </div>

    

<ul class="inline" role="navigation" aria-label="Tag list for this articles">
    <li>Tags:</li>

    <li><a href="/tags/programming.html">Software</a> </li>

</ul>


        <hr>
    <!-- nada -->




<nav role="navigation" class="table-of-contents">
    <ul>
    <li class="indent-1"><a href="#introduction">Introduction</a></li>
    <li class="indent-1"><a href="#common-test-suite-problems">Common Test Suite Problems</a></li>
    <li class="indent-2"><a href="#tests-often-emit-warnings">Tests often emit warnings</a></li>
    <li class="indent-2"><a href="#tests-often-fail">Tests often fail</a></li>
    <li class="indent-2"><a href="#there-is-little-evidence-of-organization">There is
little evidence of organization</a></li>
    <li class="indent-2"><a href="#much-of-the-testing-code-is-duplicated">Much of the testing code is duplicated</a></li>
    <li class="indent-2"><a href="#testing-fixtures-are-not-used-or-poorly-used">Testing fixtures are not used (or poorly used)</a></li>
    <li class="indent-2"><a href="#code-coverage-is-poorly-understood">Code coverage is poorly understood</a></li>
    <li class="indent-2"><a href="#they-take-far-too-long-to-run">They take far too long to run</a></li>
    <li class="indent-1"><a href="#recommendations">Recommendations</a></li>
    <li class="indent-2"><a href="#aggressively-search-for-and-remove-duplicated-tests">Aggressively search for and remove duplicated tests.</a></li>
    <li class="indent-2"><a href="#use-code-coverage-aggressively">Use code coverage aggressively</a></li>
    <li class="indent-2"><a href="#look-for-code-with-global-effects">Look for code with "global" effects</a></li>
    <li class="indent-2"><a href="#inline-hot-functions">Inline "hot" functions.</a></li>
    <li class="indent-2"><a href="#recompile-your-perl">Recompile your Perl</a></li>
    <li class="indent-2"><a href="#preload-modules">Preload modules</a></li>
    <li class="indent-2"><a href="#parallel-tests">Parallel tests</a></li>
    <li class="indent-2"><a href="#distributed-tests">Distributed tests</a></li>
    <li class="indent-2"><a href="#develcoverxcovered">Devel::CoverX::Covered</a></li>
    <li class="indent-1"><a href="#testclassmoose">Test::Class::Moose</a></li>
    </ul>
</nav>
<hr>


<p>This document is about testing applications&mdash;it's not about
how to write tests. Application test suites require a different, more
disciplined approach than library test suites. I describe common
misfeatures experienced in large application test suites and follow
with recommendations on best practices.  Much of what I describe below
is generic and applies to test suites written in any programming
language, despite many examples being written in Perl.</p>

<h1><a name="introduction"></a>Introduction</h1>

<p>I often speak with developers who take a new job and they describe
a Web site built out of a bunch of separate scripts scattered randomly
through directories, lots of duplicated code, poor use of modules,
with embedded SQL and printing HTTP headers and HTML directly. The
developers shake their head in despair, but grudgingly admit an
upside: job security. New features are time-consuming to add, changes
are difficult to implement and may have wide-ranging side-effects, and
reorganizing the codebase to have a proper separation of concerns, to
make it cheaper and safer to hack on, will take lots and lots of
time.</p>

<p>A bunch of randomly scattered scripts, no separation of concerns,
lots of duplicated code, poor use of modules, SQL embedded directly in
them? Does this sound familiar? It's your standard test suite. We're
horrified by this in the code, but don't bat an eyelash at the test
suite.</p>

<p>Part of this is because much, if not most, of the testing examples
we find focus on testing distributions, not applications. If you were
to look at the tests for my module <a
href="https://github.com/Ovid/dbix-class-easyfixture">DBIx::Class::EasyFixture</a>,
you'd see the following tests:</p>

<pre><code>00-load.t
basic.t
definitions.t
groups.t
load_multiple_fixtures.t
many_to_many.t
no_transactions.t
one_to_many.t
one_to_one.t
</code></pre>

<p>These tests were added one by one, as I added new features to
<code>DBIx::Class::EasyFixture</code> and each <code>*.t</code> file
represents (more or less) a different feature.</p>

<p>For a small distribution, this isn't too bad because it's very easy
to keep it all in your head. With only nine files, it's trivial to
glance at them, or grep them, to figure out where the relevant tests
are. Applications, however, are a different story. This is the number
of files from one of my customer's test suites:</p>

<pre><code>$ find t -type f | wc -l
288
</code></pre>

<p>That's actually fairly small. One codebase I worked on had close to
a million lines of code with thousands of test scripts. You couldn't
hold the codebase in your head, you're couldn't <em>glance</em> at the
tests to figure out what went where, nor was grepping necessarily
going to tell you as tests for particular sections of code were often
scattered around multiple test scripts.  And, of course, I regularly
hear the lament I've heard at many shops with larger codebases: where
are the tests for feature <em>X</em>? Instead of just sitting down and
writing code, the developers are hunting for the tests, wondering if
there are any tests for the feature they're working on and, if not,
trying to figure out where to put their new tests.</p>

<p>Unfortunately, this disorganization is only the start of the
problem.</p>

<h1><a name="common-test-suite-problems"></a>Common Test Suite Problems</h1>

<p>I've worked with many companies with large test suites and they
tend to share some common problems. I list them in below in the order
I try to address these problems (in other words, roughly easiest to
hardest).</p>

<ul>
    <li>Tests often emit warnings</li>
    <li>Tests often fail ("oh, that sometimes fails. Ignore it.")</li>
    <li>There is little evidence of organization</li>
    <li>Much of the testing code is duplicated</li>
    <li>Testing fixtures are not used (or poorly used)</li>
    <li>Code coverage is spotty</li>
    <li>They take far too long to run</li>
</ul>

<p>Problems are one thing, but what features do we want to see in
large-scale test suites?</p>

<ul>
    <li>Tests should be very easy to write and run</li>
    <li>They should run relatively quickly</li>
    <li>The order in which tests run should not matter</li>
    <li>Test output should be clean</li>
    <li>It should be obvious where to find tests for a particular piece of code</li>
    <li>Testing code should not be duplicated</li>
    <li>Code coverage should be able to analyze different aspects of the system</li>
</ul>

<p>Let's take a look at some of the problems and try to understand
their impacts.  While it's good to push a test suite into a desirable
state, often this is risky if the underlying problems are ignored. I
will offer recommendations for resolving each problem, but it's
important to understand that these are <em>recommendations</em>. They
may not apply to your situation.</p>

<h2><a name="tests-often-emit-warnings"></a>Tests often emit warnings</h2>

<p>This seems rather innocuous. Sure, code emits warnings and we're
used to that.  Unfortunately, we sometimes forget that warnings are
<em>warnings</em>: there might very well be something wrong. In my
time at the BBC, one of the first things I did was try to clean up all
of the warnings. One was a normal warning about use of an undefined
variable, but it was unclear to me from the code if this should be an
acceptable condition. Another developer looked at it with me and
realized that the variable should never be undefined: this warning was
masking a very serious bug in the code, but the particular condition
was not explicitly tested. By rigorously eliminating all warnings, we
found it easier to make our code more correct, and in those places
where things were dodgy, comments were inserted into the code to
explain why warnings were suppressed.  In short: the code became
easier to maintain.</p>

<p>Another issue with warnings in the test suite is that they
condition developers to ignore warnings. We get so used to them that
we stop reading them, even if something serious is going on (on a
related note, I often listen to developers complain about stack
traces, but a careful reading of a stack trace will often reveal the
exact cause of the exception). New warnings crop up, warnings change,
but developers conditioned to ignore them often overlook serious
issues with their code.</p>

<p><strong>Recommendation</strong>: Eliminate all warnings from your
test suite, but investigate each one to understand if it reflects a
serious issue. Also, some tests will capture STDERR, effectively
hiding warnings. Making warnings fatal while running tests can help to
overcome this problem.</p>

<h2><a name="tests-often-fail"></a>Tests often fail</h2>

<p>For one client, their hour-long test suite had many failing tests.
When I first started working on it, I had a developer walk me through
all of the failures and explain why they failed and why they were hard
to fix. Obviously this is a far more serious problem than warnings,
but in the minds of the developers, they were under constant deadline
pressures and as far as management was concerned, the test suite was a
luxury to keep developers happy, not "serious code." As a result,
developers learned to recognize these failures and consoled themselves
with the thought that they understood the underlying issues.</p>

<p>Of course, that's not really how it works. The developer explaining
the test failures admitted that he didn't understand some of them and
with longer test suites that routinely fail, more failures tend to
crop up. Developers conditioned to accept failures tend not to notice
them. They kick off the test suite, run and grab some coffee and later
glance over results to see if they look reasonable (that's assuming
they run all of the tests, something which often stops happening at
this point). What's worse, continuous integration tools are often
built to accomodate this. From the Jenkin's <a
href="https://wiki.jenkins-ci.org/display/JENKINS/xUnit+Plugin"
rel="nofollow">xUnit Plugin page</a>:</p>

<blockquote>
    <strong>Features</strong>
    <ul>
        <li>Records xUnit tests</li>
        <li>Mark the build unstable or fail according to threshold values</li>
    </ul>
</blockquote>

<p>In other words, there's an "acceptable" level of failure. What's
the acceptable level of failure when you debit someone's credit card,
or you're sending their medical records to someone, or you're writing
embedded software that can't be easily updated?</p>

<p>Dogmatism aside, you can make a case for acceptable levels of test
failure, but you need to understand the risks and be prepared to
accept them. However, for the purposes of this document, we'll assume
that the acceptable level of failure is zero.</p>

<p>If you absolutely cannot fix a particular failure, you should at
least mark the test as <code>TODO</code> so that the test suite can
pass. Not only does this help to guide you to a clean test suite, the
<code>TODO</code> reason is generally embedded in the test, giving the
next developer a clue what's going on.</p>

<p><strong>Recommendation</strong>: Do not allow any failing tests. If
tests fail which do not impact the correctness of the application
(such as documentation or "coding style" tests), they should be
separated from your regular tests in some manner and your systems
should recognize that it's OK for them to fail.</p> <h2><a name="there-is-little-evidence-of-organization"></a>There is
little evidence of organization</h2>

<p>As mentioned previously, a common lament amongst developers is the
difficulty of finding tests for the code they're working on. Consider
the case of <a
href="http://search.cpan.org/dist/HTML-TokeParser-Simple/"
rel="nofollow">HTML::TokeParser::Simple</a>.  The library is organized
like this:</p>

<pre><code>lib/
└── HTML
    └── TokeParser
        ├── Simple
        │   ├── Token
        │   │   ├── Comment.pm
        │   │   ├── Declaration.pm
        │   │   ├── ProcessInstruction.pm
        │   │   ├── Tag
        │   │   │   ├── End.pm
        │   │   │   └── Start.pm
        │   │   ├── Tag.pm
        │   │   └── Text.pm
        │   └── Token.pm
        └── Simple.pm
</code></pre>

<p>There's a class in there named
<code>HTML::TokeParser::Simple::Token::ProcessInstruction</code>.
Where, in the following tests, would you find the tests for process
instructions?</p>

<pre><code>t/
├── constructor.t
├── get_tag.t
├── get_token.t
├── internals.t
└── munge_html.t
</code></pre>

<p>You might think it's in the <code>get_token.t</code> test, but are
you sure? And what's that strange <code>munge_html.t</code> test? Or
the <code>internals.t</code> test? As mentioned, for a small library,
this really isn't too bad. However, what if we reorganized our tests
to reflect our library hierarchy?</p>

<pre><code>t/
└── tests/
    └── html/
        └── tokeparser/
            ├── simple/
            │   ├── token/
            │   │   ├── comment.t
            │   │   ├── declaration.t
            │   │   ├── tag/
            │   │   │   ├── end.t
            │   │   │   └── start.t
            │   │   ├── tag.t
            │   │   └── text.t
            │   └── token.t
            └── simple.t 
</code></pre>

<p>It's clear that the tests for
<code>HTML::TokeParser::Simple::Token::Tag::Start</code> are in
<code>t/tests/html/tokeparser/simple/token/tag/start.t</code>. And you
can see easily that there is no file for
<code>processinstruction.t</code>. This test organization not only
makes it easy to find where your tests are, it's also easy to program
your editor to automatically switch between the code and the tests for
the code. For large test suites, this saves a huge amount of time.
When I reorganized the test suite of the BBC's central metadata
repository, <a
href="http://www.bbc.co.uk/blogs/bbcinternet/2009/02/what_is_pips.html"
rel="nofollow">PIPs</a>, I followed a similar pattern and it made our
life much easier.</p>

<p>(<strong>Note</strong>: the comment about programming your editor
is important. Effective use of your editor/IDE is one of the most
powerful tools in a developer's toolbox.)</p>

<p>Of course, your test suite could easily be more complicated and
your top-level directories inside of your test directory may be
structured differently:</p>

<pre><code>t/
├── unit/
├── integration/
├── api/
└── web/
</code></pre>

<p><strong>Recommendation</strong>: Organize your test files to have a
predictable, discoverable structure. The test suite should be much
easier to work with.</p>

<h2><a name="much-of-the-testing-code-is-duplicated"></a>Much of the testing code is duplicated</h2>

<p>We're aghast that that people routinely cut-n-paste their
application code, but we don't even notice when people do this in
their test code. More than once I've worked on a test suite with a
significant logic change and I've had to find this duplicated code and
either change it many places or try to refactor it so that it's in a
single place and then change it. We already know why duplicated code
is bad, I'm unsure why we tolerate this in test suites.</p>

<p>Much of my work in tests has been to reduce this duplication. For
example, many test scripts list the same set of modules at the top. I
did a heuristic analysis of tests on the CPAN and chose the most
popular testing modules and that allowed me to change this:</p>

<pre><code>use strict;
use warnings;
use Test::Exception;
use Test::Differences;
use Test::Deep;
use Test::Warn;
use Test::More tests =&gt; 42;
</code></pre>

<p>To this:</p>

<pre><code>use Test::Most tests =&gt; 42;</code></pre>

<p>You can easily use similar strategies to bundle up common testing
modules into a single testing module that all of your tests use. Less
boilerplate and you can easily dive into testing.</p>

<p>Or as a more egregious example, I often see something like this (a
silly example just for illustration purposes):</p>

<pre><code>set_up_some_data($id);
my $object = Object-&gt;new($id);

is $object-&gt;attr1, $expected1, 'attr1 works';
is $object-&gt;attr2, $expected2, 'attr2 works';
is $object-&gt;attr3, $expected3, 'attr3 works';
is $object-&gt;attr4, $expected4, 'attr4 works';
is $object-&gt;attr5, $expected5, 'attr5 works';
</code></pre>

<p>And then a few lines later:</p>

<pre><code>set_up_some_data($new_id);
$object = Object-&gt;new($new_id);

is $object-&gt;attr1, $new_expected1, 'attr1 works';
is $object-&gt;attr2, $new_expected2, 'attr2 works';
is $object-&gt;attr3, $new_expected3, 'attr3 works';
is $object-&gt;attr4, $new_expected4, 'attr4 works';
is $object-&gt;attr5, $new_expected5, 'attr5 works';
</code></pre>

<p>And then a few lines later, the same thing ...</p>

<p>And in another test file, the same thing ...</p>

<p>Put that in its own test function and wrap those attribute tests in
a loop. If this pattern is repeated in different test files, put it in
a custom test library:</p>

<pre><code>sub test_fetching_by_id ( $class, $id, $tests ) {
    my $object = $class-&gt;new($id);

    # this causes diagnostics to display the file and line number of the
    # caller on failure, rather than reporting *this* file and line number
    local $Test::Builder::Level = $Test::Builder::Level + 1;

    foreach my $test (@$tests) {
        my ( $attribute, $expected ) = @$test;
        is $object-&gt;$attribute, $expected, "$attribute works for $class $id";
    }
}
</code></pre>

<p>And then you call it like this:</p>
<pre><code>my @id_tests = (
    {
        id    =&gt; $id,
        tests =&gt; [
            [ attr1 =&gt; $expected1 ],
            [ attr2 =&gt; $expected2 ],
            [ attr3 =&gt; $expected3 ],
            [ attr4 =&gt; $expected4 ],
            [ attr5 =&gt; $expected5 ],
        ]
    },
    {
        id     =&gt; $new_id,
        tests  =&gt; [
            [ attr1 =&gt; $new_expected1 ],
            [ attr2 =&gt; $new_expected2 ],
            [ attr3 =&gt; $new_expected3 ],
            [ attr4 =&gt; $new_expected4 ],
            [ attr5 =&gt; $new_expected5 ],
        ]
    },
);

for my $test ( @id_tests ){
    test_fetching_by_id( 'Object', $test-&gt;{id}, $tests-&gt;{test} );
}
</code></pre>

<p>This is a cleanly refactored data-driven approach. By not repeating
yourself, if you need to test new attributes, you can just add an
extra line to the data structures and the code remains the same. Or,
if you need to change the logic, you only have one spot in your code
where this is done. Once a developer understands the
<code>test_fetching_by_id()</code> function, they can reuse this
understanding in multiple places. Further, it makes it easier to find
patterns in your code and any competent programmer is always on the
lookout for patterns because those are signposts leading to cleaner
designs.</p>

<p><strong>Recommendation</strong>: Keep your test code as clean as
your application code.</p>

<h2><a name="testing-fixtures-are-not-used-or-poorly-used"></a>Testing fixtures are not used (or poorly used)</h2>

<p>One difference between your application code and the test suite is
in an application, we often have no idea what the data will be and we
try to have a clean separation of data and code.</p>

<p>In your test suite, we also want a clean separation of data and
code (in my experience, this is very hit-or-miss), but we often
<em>need</em> to know the data we have. We set up data to run tests
against to ensure that we can test various conditions. Can we give a
customer a birthday discount if they were born February 29th? Can a
customer with an overdue library book check out another?  If our
employee number is no longer in the database, is our code properly
deleted, along with the backups and the git history erased?
(kidding!)</p>

<p>When we set up the data for these known conditions under which to
test, we call the data a <a
href="http://en.wikipedia.org/wiki/Test_fixture" rel="nofollow">test
fixture</a>.  Test fixtures, when properly designed, allow us generate
clean, understandable tests and make it easy to write tests for
unusual conditions that may otherwise be hard to analyze.</p>

<p>There are several common anti-patterns I see in fixtures.</p>

<ul>
    <li>Hard to set up and use</li>
    <li>Adding them to the database and not rolling them back</li>
    <li>Loading all your test data at once with no granularity</li>
</ul>

<p>In reviewing various fixture modules on the CPAN and for clients I
have worked with, much of the above routinely holds true. On top of
that, documentation is often rather sparse or non-existent. Here's a
(pseudo-code) example of an almost undocumented fixture system for one
client I worked with and it exemplified common issues in this
area.</p>

<pre><code>load_fixture(
    database =&gt; 'sales',
    client   =&gt; $client_id,
    datasets =&gt; [qw/ customers orders items order_items referrals /],
);
</code></pre>

<p>This had several problems, all of which could be easily corrected
<em>as code</em>, but they built a test suite around these problems
and had backed themselves into a corner, making their test suite
dependent on bad behavior.</p>

<p>The business case is that my client had a product serving multiple
customers and each customer would have multiple separate databases. In
the above, client <em>$client_id</em> connects to their sales database
and we load several test datasets and run tests against them. However,
loading of data was not done in a transaction, meaning that there was
no isolation between different test cases in the same process. More
than once I caught issues where running an individual test case would
often fail because it depended on data loaded by a different test
case, but it wasn't always clear which test cases were coupled with
which.</p>

<p>Another issue is that fixtures were not fine-tuned to address
particular test cases. Instead, if you loaded "customers" or
"referrals", you got <em>all</em> of them in the database. Do you need
a database with a single customer with a single order and only one
order item on it to test that obscure bug that occurs when a client
first uses your software? There really wasn't any clean way of doing
that; data was loaded in an "all or nothing" context. Even if you
violated the paradigm and tried to create fine-tuned fixtures, it was
very hard to write them due to the obscure, undocumented format needed
to craft the data files for them.</p>

<p>Because transactions were not used and changes could not be rolled
back, each <code>*.t</code> file would rebuild its own test database,
a very slow process. Further, due to lack of documentation about the
fixtures, it was often difficult to figure out which combination of
fixtures to load to test a given feature. Part of this is simply due
to the complex nature of the business rules, but the core issues
stemmed from a poor understanding of fixtures. This client now has
multiple large, slow test suites, spread across multiple repositories,
all of which constantly tear down and set up databases and load large
amounts of data. The test suites are both slow and fragile The time
and expense to fix this problem is considerable due to how long
they've pushed forward with this substandard setup.</p>

<p>What you generally want is the ability to easily create
understandable fixtures which are loaded in a transaction, tests are
run, and then changes are rolled back.  The fixtures need to be
fine-grained so you can tune them for a particular test case.</p>

<p>One attempt I've made to fix this situation is releasing <a
href="http://search.cpan.org/dist/DBIx-Class-EasyFixture/lib/DBIx/Class/EasyFixture.pm"
rel="nofollow">DBIx::Class::EasyFixture</a>, along with <a
href="http://search.cpan.org/dist/DBIx-Class-EasyFixture/lib/DBIx/Class/EasyFixture/Tutorial.pm"
rel="nofollow">a tutorial</a>.  It does rely on
<code>DBIx::Class</code>, the most popular ORM for Perl. This will
likely make it unsuitable for some use cases.</p>

<p>Using them is very simple:</p>

<pre><code>my $fixtures = DBIx::Class::EasyFixture-&gt;new(schema =&gt; $schema);
$fixtures-&gt;load('customer_with_order_without_items');

# run your tests

$fixtures-&gt;unload; # also unloads when out of scope
</code></pre>

<p>For the customer's code, we could satisfy the different database
requirements by passing in different schemas. Other (well-documented)
solutions, particularly those which are pure <code>DBI</code> based
are welcome in this area.</p>

<p><strong>Recommendation</strong>: Fine-grained, well-documented
fixtures which are easy to create and easy to clean up.</p>

<h2><a name="code-coverage-is-poorly-understood"></a>Code coverage is poorly understood</h2>

<p>Consider the following code:</p>
<pre><code>float recip(float number) {
    return 1.0 / number;
}
</code></pre>

<p>And a sample test:</p>

<pre><code>assert recip(2.0) returns .5;</code></pre>

<p>Congratulations! You now have 100% code coverage of that function.</p>

<p>For a statically typed language, I'm probably going to be
moderately comfortable with that test. Alas, for dynamically typed
languages we're fooling ourselves. An equivalent function in Perl will
pass that test if we use <code>recip("2 apples")</code> as the
argument. And what happens if we pass a file handle? And would a
Unicode number work? What happens if we pass no arguments?  Perl is
powerful and lets us write code quickly, but there's a price: it
expects us to know what we're doing and passing unexpected kinds of
data is a very common source of errors, but one that 100% code
coverage will never (no pun intended) uncover. This can lead to false
confidence.</p>

<p>To work around false confidence in your code, always assume that
you write applications to create things and you write tests to destroy
them. Testing is, and should be, an act of violence. If you're not
breaking anything with your tests, you're probably doing it wrong.</p>

<p>Or what if you have that code in a huge test suite, but it's dead
code? We tend to blindly run code coverage over our entire test suite,
never considering whether or not we're testing dead code. This is
because we slop our unit, integration, API and other tests all
together.</p>

<p>Or consider the following test case:</p>
<pre><code>sub test_forum : Tests(1) ($self) {
    my $site = $self-&gt;test_website;
    $site-&gt;login($user, $pass);
    $site-&gt;get('/forum');
    $site-&gt;follow_link( text =&gt; 'Off Topic' );
    $site-&gt;post_ok({
        title =&gt; 'What is this?',
        body  =&gt; 'This is a test'.
    }, 'We should be able to post to the forum');
}
</code></pre>

<p><code>Devel::Cover</code> doesn't know which code is test code and
which is not.  <code>Devel::Cover</code> merely tells you if your
application code was exercised in your tests. <a
href="http://search.cpan.org/dist/Devel-Cover/lib/Devel/Cover.pm#UNCOVERABLE_CRITERIA"
rel="nofollow">You can annotate your code with "uncoverable"
directives</a> to tell <code>Devel::Cover</code> to ignore the
following code, but that potentially means sprinkling your code with
annotations all over the place.</p>

<p>There are multiple strategies to deal with this. One of the
simplest is to merely run your code coverage tools over the
public-facing portions of your code, such as web or API tests. If you
find uncovered code, you either have code that is not fully tested (in
the sense that you don't know if your API can really use that code)
or, if you cannot write an API test to reach that code, investigate if
it is dead code.</p>

<p>You can do this by grouping your tests into subdirectories:</p>
<pre><code>t/
    |--api/
    |--integration/
    `--unit/
</code></pre>

<p>Alternatively, if you use <code>Test::Class::Moose</code>, you can
tag your tests and only run coverage over tests including the tags you
wish to test:</p>

<pre><code>My::Test::Class::Moose-&gt;new({
    include_tags =&gt; [qw/api/],
})-&gt;runtests;
</code></pre>

<p>If you start tagging your tests by the subsystems they are testing,
you can then start running code coverage on specific subsystems to
determine which ones are poorly tested.</p>

<p><strong>Recommendation</strong>: Run coverage over public-facing
code and on different subsystems to find poor coverage.</p>

<h2><a name="they-take-far-too-long-to-run"></a>They take far too long to run</h2>

<p>The problem with long-running test suites is well known, but it's
worth covering this again here. These are problems that others have
discussed and that I have also personally experienced many times.</p>

<p><img src="/static/images/tests_are_running.png" alt="Cartoon showing
programmer's sword fighting while waiting for their test suite to compile."
style="max-width:100%;"></p>

<p><em>With apologies to <a href="http://xkcd.com/303/" rel="nofollow">XKCD</a></em></p>

<p>In the best case scenario for developers who always run that
long-running test suite, expensive developer time is wasted while the
test suite is running.  When they launch that hour-long (or more) test
suite, they frequently take a break, talk to (read: interrupt) other
developers, check their Facebook, or do any number of things which
equate to "not writing software." Yes, some of those things involve
meetings or research, but meetings don't conveniently schedule
themselves when we run tests and for mature products (those which are
more likely to have long-running test suites), there's often not that
much research we really need to do.</p>

<p>Here are some of the issues with long-running test suites:</p>

<ul>
    <li>Expensive developer time is wasted while the test suite runs</li>
    <li>Developers often don't run the entire test suite</li>
    <li>Expensive code coverage is not generated as a result</li>
    <li>Code is fragile as a result</li>
</ul>

<p>What I find particularly curious is that we accept this state of
affairs. Even a back-of-the-envelope calculation can quickly show
significant productivity benefits that will pay off in the long run by
taking care of our test suite.  <a
href="http://www.slideshare.net/Ovid/turbo-charged-test-suites-presentation"
rel="nofollow">I once reduced a BBC test suite's run time from one
hour and twenty minutes down to twelve minutes</a> (<em>Note: today I
use a saner approach that results in similar or greater performance
benefits</em>).  We had six developers on that team. When the test
suite took over an hour to run, they often didn't run the test suite.
They would run tests on their section of code and push their code when
they were comfortable with it. This led to other developers finding
buggy code and wasting time trying to figure out how they broken it
when, in fact, someone else broke the code.</p>

<p>But let's assume each developer was running the test suite at least
once a day (I'm careful about testing and often ran mine twice a day).
By cutting test suite run time by over an hour, we reclaimed a
<em>full day</em> of developer productivity every day! Even if it
takes a developer a month to increase perfomance by that amount it
pays for itself many times over very quickly.  Why would you not do
this?  As a business owner, wouldn't you want your developers to save
time on their test suite so they can create features faster for
you?</p>

<p>There are several reasons why this is difficult. Tasking a
developer with a block of time to speed up a test suite means the
developer is not creating user-visible features during that time. For
larger test suites, it's often impossible to know in advance just how
much time you can save or how long it will take you to reach your
goal. In most companies I've worked with, the people who can make the
decision to speed up the test suite are often not the people feeling
the pain. Productivity and quality decrease slowly over time, leading
to the <a href="http://en.wikipedia.org/wiki/Boiling_frog"
rel="nofollow">boiling frog problem</a>.</p>

<p>What's worse: in order to speed up your test suite without
affecting behavior, the test suite often has to be "fixed"
(eliminating warnings, failures, and reducing duplication) to ensure
that no behavior has been changed during the refactor.</p>

<p>Finally, some developers simply don't have the background necessary
to implement performance optimizations. While performance profiles
such as Perl's <a
href="http://search.cpan.org/dist/Devel-NYTProf/lib/Devel/NYTProf.pm"
rel="nofollow">Devel::NYTProf</a> can easily point out problem areas
in the code, it's not always clear how to overcome the discovered
limitations.</p>

<p>The single biggest factor in poor test suite performance for
applications is frequently I/O. In particular, working with the
database tends to be a bottleneck and there's only so much database
tuning that can be done. After you've profiled your SQL and optimized
it, several database-related optimizations which can be considered
are:</p>

<ol>
  <li>Using transactions to clean up your database rather than rebuilding the database</li>
  <li>Only connect to the database once per test suite (hard when you're using a separate process per test file)</li>
  <li>If you must rebuild the database, maintain a pool of test databases and assign them as needed, rebuilding used ones in the background</li>
  <li>Use smaller database fixtures instead of loading everything at once</li>
</ol>

<p>After you've done all you can to improve your database access, you
may find that your test suite is "fast enough", but if you wish to go
further, there are several steps you can take.</p>

<h1><a name="recommendations"></a>Recommendations</h1>

<h2><a name="aggressively-search-for-and-remove-duplicated-tests"></a>Aggressively search for and remove duplicated tests.</h2>

<p>For poorly organized test suites, developers sometimes make the
mistake of putting tests for something in a new <code>*.t</code> file
or add them to a different <code>*.t</code> file, even if related
tests already exist. This strategy can be time-consuming and often
does not result in quick wins.</p>

<h2><a name="use-code-coverage-aggressively"></a>Use code coverage aggressively</h2>

<p>For one test suite, I found that we were using a pure Perl
implementation of JSON. As the test suite used JSON extensively,
switching to <a href="http://search.cpan.org/dist/JSON-XS/XS.pm"
rel="nofollow">JSON::XS</a> gave us a nice performance boost. We may
not have noticed that if we hadn't been profiling our code.</p>

<h2><a name="look-for-code-with-global-effects"></a>Look for code with "global" effects</h2>

<p>On one test suite, I ensured that <code>Universal::isa</code> and
<code>Universal::can</code> cannot be loaded. It was a quick fix and
sped up the test suite by 2% (several small accumulations of
improvements can add up quickly).</p>

<h2><a name="inline-hot-functions"></a>Inline "hot" functions.</h2>

<p>Consider the following code which runs in about 3.2 seconds on my
computer:</p>

<pre><code>#!/usr/bin/env perl

use strict;
use warnings;
no warnings 'recursion';

for my $i ( 1 .. 40 ) {
    for my $j ( 1 .. $i**2 ) {
        my $y = factorial($j);
    }
}

sub factorial {
    my $num = shift;
    return 1 if $num &lt;= 1;
    return $num * factorial($num - 1);
}
</code></pre>

<p>By rewriting the recursive function as a loop, the code takes about
.87 seconds:</p>

<pre><code>sub factorial {
    my $num = shift;
    return 1 if $num &lt;= 1;
    $num *= $_ for 2 .. $num - 1;
    return $num;
}
</code></pre>

<p>By inlining the calculation, the code completes in .69 seconds:</p>

<pre><code>for my $i ( 1 .. 40 ) {
    for my $j ( 1 .. $i**2 ) {
        my $y = $j;
        if ( $y &gt; 1 ) {
            $y *= $_ for 2 .. $y - 1;
        }
    }
}
</code></pre>

<p>In other words, in our trivial example, the inlined behavior is roughly 20%
faster than the iterative function and 80% faster than the recursive function.</p>


<h2><a name="recompile-your-perl"></a>Recompile your Perl</h2>

<p>You may wish to recompile your Perl to gain a performance
improvement. Many Linux distributions ship with a threaded Perl by
default. Depending on the version of Perl you ship with, you can gain
performance improvements of up to 30% by recompiling without threads.
Of course, if you use threads, you'll feel very stupid for doing this.
However, if you don't make heavy use of threads, switching to a
forking model for the threaded code may make the recompile worth it.
Naturally, you'll need to heavily benchmark your code (preferably
under production-like loads) to understand the trade-offs here.</p>

<h2><a name="preload-modules"></a>Preload modules</h2>

<p>If your codebase makes heavy use of modules that are slow to load,
such as <code>Moose</code>, <code>Catalyst</code>,
<code>DBIx::Class</code> and others, preloading them might help.  <a
href="http://search.cpan.org/%7Emiyagawa/forkprove-v0.4.9/script/forkprove"
rel="nofollow">forkprove</a> is a utility written by Tatsuhiko
Miyagawa that allows you to preload slow-loading modules and then
forks off multiple processes to run your tests.  Using this tool, <a
href="http://blogs.perl.org/users/ovid/2013/12/merry-christmas-parallel-testing-with-testclassmoose-has-arrived.html"
rel="nofollow">I reduced one sample test suite's run time from 12
minutes to about a minute</a>.  Unfortunately, <code>forkprove</code>
doesn't allow schedules, a key component often needed for larger test
suites. I'll explain that in the next section.</p>

<h2><a name="parallel-tests"></a>Parallel tests</h2>

<p>Running tests in parallel is tricky. Some tests simply
<em>can't</em> be run with other tests. Usually these are tests which
alter global state in some manner that other processes will pick up,
or might cause resource starvation of some kind.</p>

<p>Or some tests <em>can</em> be run in parallel with other tests, but
if several tests are updating the same records in the database at the
same time, locking behavior might slow down the tests
considerably.</p>

<p>Or maybe you're running 4 jobs, but all of your slowest tests are
grouped in the same job: not good.</p>

<p>To deal with this, you can create a schedule that assigns different
tests to different jobs, based on a set of criteria, and then puts
tests which cannot run in parallel in a single job that runs after the
others have completed.</p>

<p>You can use <a
href="http://search.cpan.org/dist/Test-Harness/lib/TAP/Parser/Scheduler.pm"
rel="nofollow">TAP::Parser::Scheduler</a> to create an effective
parallel testing setup. You can use this with
<code>TAP::Parser::Multiplexer</code> to create your parallel tests.
Unfortunately, as of this writing there's a bug in the Multiplexer
whereby it uses <code>select</code> in a loop to read the parser
output. If one parser blocks, none of the other output is read.
Further, the schedule must be created prior to loading your test code,
meaning that if your tests would prefer a different schedule, you're
out of luck. Also, <code>make test</code> currently doesn't handle
this well. There is work being done by David Golden to alleviate this
problem.</p>

<p>My preferred solution is to use <a
href="http://search.cpan.org/dist/Test-Class-Moose/"
rel="nofollow">Test::Class::Moose</a>. That has built-in parallel
testing and writing schedules is very easy. Further, different test
cases can simply use a <code>Tags(noparallel)</code> attribute to
ensure that they're run sequentially after the parallel tests.</p>

<p>Aside from the regular benefits of <code>Test::Class::Moose</code>,
an interesting benefit of this module is that it loads all of your
test and application code into a single process and <em>then</em>
forks off subprocesses. As a result, your code is loaded once and only
once. Alternate strategies which try to fork before loading your code
might still cause the code to be loaded multiple times.</p>

<p>I have used this strategy to reduce a <a
href="http://blogs.perl.org/users/ovid/2013/12/merry-christmas-parallel-testing-with-testclassmoose-has-arrived.html"
rel="nofollow">12 minute test suite to 30 seconds</a>.</p>

<h2><a name="distributed-tests"></a>Distributed tests</h2>

<p>Though I haven't used this module, Alex Vandiver has written <a
href="http://search.cpan.org/dist/TAP-Harness-Remote/lib/TAP/Harness/Remote.pm"
rel="nofollow">TAP::Harness::Remote</a>.  This module allows you to
rsync directory trees to multiple servers and run tests on those
servers. Obviously, this requires multiple servers.</p>

<p>If you want to roll your own version of this, I've also released <a
href="http://search.cpan.org/dist/TAP-Stream/"
rel="nofollow">TAP::Stream</a>, a module that allows you to take
streams (the text, actually) of TAP from multiple sources and combine
them into a single TAP document.</p>

<h2><a name="develcoverxcovered"></a>Devel::CoverX::Covered</h2>

<p>There is yet another interesting strategy: only run tests that
exercise the code that you're changing. Johan Lindström wrote <a
href="http://search.cpan.org/dist/Devel-CoverX-Covered/"
rel="nofollow">Devel::CoverX::Covered</a>.  This modules is used in
conjunction with Paul Johnson's <a
href="http://search.cpan.org/dist/Devel-Cover/"
rel="nofollow">Devel::Cover</a> to identify all the places in your
tests which cover a particular piece of code. In the past, I've
written tools for vim to read this data and only run relevant tests.
This is a generally useful approach, but there are a couple of
pitfalls.</p>

<p>First, if you test suite takes a long time to run, it will take
much, much longer to run with <code>Devel::Cover</code>. As a result,
I recommend that this be used with a special nightly "cover build" and
have the results synched back to the developers.</p>

<p>Second, when changing code, it's easy to change which tests cover
your code, leading to times when this technique won't cover your
actual changes thoroughly. In practice, this hasn't been a problem for
me, but I've not used it enough to say that with confidence.</p>

<p><strong>Recommendation</strong>: Don't settle for slow test suites.
Pick a goal and work to achieving that goal (it's easy to keep
optimizing for too long and start getting diminishing marginal
returns).</p>

<h1><a name="testclassmoose"></a>Test::Class::Moose</h1>

<p>If you start creating a large Web site, do you start writing a
bunch of individual scripts, each designed to handle one URL and each
handling their own database access and printing their output directly
to STDOUT? Of course not. Today, professional developers reach for
Sinatra, Seaside, Catalyst, Ruby on Rails or other Web frameworks.
They take a bit more time to set up and configure, but we know they
generally save more time in the long run. Why wouldn't you do that
with your test suite?</p>

<p>If you're using Perl, many of the problems listed in this document
can be avoided by switching to <code>Test::Class::Moose</code>. This
is a testing framework I designed to make it very easy to test
applications. Once you understand it, it's actually easy to use for
testing libraries, but it really shines for application testing.</p>

<p>Note that I now regret putting <code>Moose</code> in the name.
<code>Test::Class::Moose</code> is a rewrite of
<code>Test::Class</code> using <code>Moose</code>, but it's
<em>not</em> limited to testing <code>Moose</code> applications. It
uses <code>Moose</code> because internally it relies on the
<code>Moose</code> meta-object protocol for introspection.</p>

<p>Out of the box you get:</p>

<ul>
    <li>Reporting</li>
    <li>Parallel tests (which optionally accepts a custom schedule)</li>
    <li>Tagging tests (slice and dice your test suite!)</li>
    <li>Test inheritance (xUnit for the win!)</li>
    <li>Full Moose support</li>
    <li>Test control methods (startup, setup, teardown, shutdown)</li>
    <li>Extensibility</li>
    <li>All the testing functions and behavior from <code>Test::Most</code></li>
</ul>

<p>To learn about xUnit testing in Perl, you may wish to read a
five-part tutorial I published at Modern Perl Books:</p>

<ol>
    <li><a href="http://www.modernperlbooks.com/mt/2009/03/organizing-test-suites-with-testclass.html" rel="nofollow">Organizing test suites with Test::Class</a></li>
    <li><a href="http://www.modernperlbooks.com/mt/2009/03/reusing-test-code-with-testclass.html" rel="nofollow">Reusing test code</a></li>
    <li><a href="http://www.modernperlbooks.com/mt/2009/03/making-your-testing-life-easier.html" rel="nofollow">Making your testing life easier</a></li>
    <li><a href="http://www.modernperlbooks.com/mt/2009/03/using-test-control-methods-with-testclass.html" rel="nofollow">Using test control methods</a></li>
    <li><a href="http://www.modernperlbooks.com/mt/2009/03/working-with-testclass-test-suites.html" rel="nofollow">Working with Test::Class test suites</a></li>
</ol>

<p>That tutorial is slightly out of date (I wrote it a few years ago),
but it explains effective use of <code>Test::Class</code> and some
common anti-patterns when using it.</p>

<p>Doug Bell has started <a
href="http://search.cpan.org/dist/Test-Class-Moose/lib/Test/Class/Moose/Tutorial.pm"
rel="nofollow">a tutorial for Test::Class::Moose</a>.  That also needs
updating, but between those and reading the
<code>Test::Class::Moose</code> documentation, you should be able to
get up to speed fairly quickly.</p>



          <p><strong>Please leave a comment below!</strong></p>



<script type="text/javascript">
    // estimated reading time
    // via https://dev.to/michaelburrows/calculate-the-estimated-reading-time-of-an-article-using-javascript-2k9l

    function readingTime() {
        const text  = document.getElementById("article").innerText;

        // The average eighth grader in the US reads about 250 words a minute, but your average
        // university student reads at almost twice that speed. However, my technical articles
        // are more in-depth and can take some time to think about, so be fair to the reader
        // and assume they'll take a bit longer for that.
        const wpm   = "articles" === "articles" ? 250 : 350;
        const words = text.trim().split(/\s+/).length;
        const time  = Math.ceil(words / wpm);
        document.getElementById("time").innerText = time;
    }
    readingTime();
</script>

<!-- map images pop up when you click them -->
<div id="overlay" class="overlay">
    <img id="overlayImage" src="" alt="Full-size image">
</div>
<script>
    function showOverlay(img) {
        const overlay = document.getElementById('overlay');
        const overlayImage = document.getElementById('overlayImage');
        overlayImage.src = img.src;
        overlayImage.alt = img.alt;
        overlay.style.display = 'block';
    }

    function hideOverlay() {
        const overlay = document.getElementById('overlay');
        overlay.style.display = 'none';
    }

    document.addEventListener('DOMContentLoaded', function() {
        const images = document.querySelectorAll('.image-container img');
        images.forEach(img => {
            img.addEventListener('click', function() {
                showOverlay(this);
            });
        });

        const overlay = document.getElementById('overlay');
        overlay.addEventListener('click', hideOverlay);
    });
</script>

<hr>
    <div class="prevNext">
        
        <a href="/#" class="prevPost"></a>
        
        <a href="/#" class="nextPost"></a>
    </div>

<hr>

          <p>If you'd like top-notch consulting or training, <a
          href="mailto:curtis.poe@gmail.com">email me</a> and let's discuss
          how I can help you. Read my <a href="/hireme.html">hire me</a> page
          to learn more about my background.</p>
        </div>
    </div>
<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row">
      <div class="two columns">
        <p></p>
      </div>
      <div class="ten columns">
        <hr>
        <p>Copyright &copy; 2018-2025 by Curtis “Ovid” Poe.</p>
      </div>
    </div>
        <div id="disqus_thread"></div>
    <div class="row">
      <div class="twelve columns">
      
        <script>
        var disqus_config = function () {
            this.page.url        = "https://curtispoe.org/articles/zen-of-test-suites.html";
            this.page.identifier = "articles/zen-of-test-suites";
            this.page.title      = "The Zen of Test Suites";
        };
        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://https-ovid-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      
        </div>
    </div>
    </div>
    
	
</body>
</html>

